Processing Data
===============

**Goals**

This stage involves transforming the raw data to a state which is able to be analysed and observed in the context of this project. This stage involves several chronological steps which ensures that the data used is suitable for the conclusions drawn. 


**Tasks**

 1. [Remove Human DNA](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#3-human-dna-removal)
 2. [Deduplicate the Reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#1-deduplicating-data)
 3. [Remove Low Komplexity Reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#2-low-complexity-removal)
 4. [Check the Quality of remaining reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#4-quality-assessment)
 5. [Combining Reads Together](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#5-combining-reads-together)
 6. [Sample Read Counts](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#6-sample-read-counts)


## 1. Human DNA removal 

### This step used 'kneaddata' which was downloaded through the following scripts:

This step is used to remove and human DNA from the sequences. this will ensure that the sequences ultimately used contain only viral, bacterial and archaic sequences, so as to ensure that there is lowered/no contamination present in the final dataset

#### kneaddata was installed via:

    conda create -n knead kneaddata p=3
    
#### To  run the reads the following commands were entered:

    source activate knead
    module load parallel
    sbatch (Jobscript name)
    
#### The script to be entered is:

    #!/bin/bash
    #SBATCH -p sbatch
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=10:00:00
    #SBATCH --mem=64GB
    
    #Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    parallel -j  1  'kneaddata --link -i {1(links to input for read 1)} -1 {2(links to input for read 2)} \
    -o /(output directory)/ \
    -db /(database entry for homo sapiens)/ \
    --remove-intermediate-output --bypass-trim -p 10 -t 32'\
    ::: /(input for read 1) \
    ::: /(input for read 2) \
    
#### Program version used:

## 2. Deduplicating Data
This step uses a program known as 'dedupe'. this program was downloaded using miniconda through the following command: 

This step involves the removal of duplicate copies from the sequences. This will reduce the amount of reads and will leave each read as a unique read. 

### The scripts used to complete this step are listed below:

#### Dedupe was installed via:
 
    conda create -c bioconda -n BBmap bbmap
  
#### To run the reads the following commands were entered:
   
    source activate BBmap
    sbatch (jobscript name) - run from directory containing fastq. files

#### The script to be entered is: 

    #!/bin/bash
    #SBATCH -p highmem
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=5:00:00
    #SBATCH --mem=250GB
    
    # Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    For i in *fastq;
    Do
	    dedupe2.sh in=$i \
	    out=(data output directory)/${i%.fa*}_deduped.fastq \
	    ac=f;
	    Done 

This resulted in all remaining reads being located in the dedupe output directory. It was then possible to quantify the number of reads that were lost during the deduplication process, allowing observation of the proportion of duplicated reads initially in the dataset.

#### Program Version used: 38.69


## 3. Low Complexity Removal
This step was completed using a program known as Komplexity which is able to be downloaded via miniconda using the following command: 

This step involves the removal of any samples that have low complexity. The removal of low complexity sequences will result in more concise matching to the annotated sequences further down the processing pipeline. 

* I would recommend before running the following commands, copying the output of the dedupe into a seperate and safe file as komplexity does not make a new output directory, rather replaces the input through filtering
* komplexity also does not work on any zipped files. To unzip any files use the following command:

        gunzip *fastq.gz


#### Komplexity was installed via:

    conda install -eclarke -n Komplex komplexity

#### To run the reads the following commands were entered:

    source activate Komplex
    sbatch (jobscript name)
    
#### The script to be entered is:

    #!/bin/bash
    #SBATCH -p highmem
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=3:00:00
    #SBATCH --mem=200GB
    
    # Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    For fq in (input directory)/*.fastq;
    do
    kz --filter < $fq > ${fq/.fastq/-kz.fastq};
    done
    
This resulted in a only a return of high complexity files. It was then possible to quantify the number of reads lost during the removal of low complexity reads phase to determine the proportion of the dataset that was low proportion.

#### Program version used: 0.3.6

## 4. Quality Assessment
This step was compleed using a FastQ and MultiQC. These were able to be loaded to miniconda using the following commands:

This step was used to assess the quality of the sequences so as to ensure that the sequences are appropriate to continue with. This step allows for the identification of quality based on the following elements: 

### This step was completed using the following scripts:

#### Fastq was already installed as part of the ())( package: To confirm that it is already present run the following command:

    module spider FastQC

#### To run the reads the following commands were entered:

    module load FastQC

#### The script to be entered is:

    fastqc -t -o (output)/ (input)/*
    
#### Program version used:

## 5. Combining Reads Together

This step is used to concatenate the paired reads together so that each pair of reads is combined under one sample name.

### The scripts used to complete this step are listed below
Open a screen using:

    screen -S (name of screen)
To then run the command write the following:

    for fq in *paired_1_deduped-kz.fastq; do cat $fq ${fq/paired_1/paired_2} > ${fq/paired_1/catted}; done 
This will result in each of the paired reads becoming combined and thus allows for them to be run through the Malt-Run program. 

 - no modules need to be loaded during this code. 
 
 ## 6. Sample Read Counts

This step has been included in order to observe the numbers of read counts following each of the processing steps in order to observe the proportions of loss obtained through the various filtering steps. This step should be completed on the raw data as well as following the completion of Knead, Dedupe and Komplexity.

### In order to complete these read counts follow the steps below:

In order to create files containing the names of the samples and the number of reads contained in each of the samples run the following command: 

    for fastq in *.fastq.gz; do echo $fastq >> SampleName.txt && zcat $fastq | echo $((`wc -l`/4)) >> NumberOfReads.txt; done

In order to then merge these files together in a column format, run the following command:

    paste SampleName.txt NumberOfReads.txt > SampleCount.txt | column -s $’\t’ -t


