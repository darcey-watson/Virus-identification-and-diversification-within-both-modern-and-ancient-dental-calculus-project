Processing Data
===============

**Goals**

This stage involves transforming the raw data to a state which is able to be analysed and observed in the context of this project. This stage involves several chronological steps which ensures that the data used is suitable for the conclusions drawn. 


**Tasks**

 1. [Deduplicate the Reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#1-deduplicating-data)
 2. [Remove Low Komplexity Reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#2-low-complexity-removal)
 3. [Remove Human DNA](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#3-human-dna-removal)
 4. [Check the Quality of remaining reads](https://github.com/darcey-watson/Viral-Content-Project/blob/master/1.%20Processing%20Data.md#4-quality-assessment)


## 1. Deduplicating Data
This step uses a program known as 'dedupe'. this program was downloaded using miniconda through the following command: 

This step involves the removal of duplicate copies from the sequences. This will reduce the amount of reads and will leave each read as a unique read. 

### The scripts used to complete this step are listed below:

#### Dedupe was installed via:
 
    conda create -c bioconda -n BBmap bbmap
  
#### To run the reads the following commands were entered:
   
    source activate BBmap
    sbatch (jobscript name) - run from directory containing fastq. files

#### The script to be entered is: 

    #!/bin/bash
    #SBATCH -p highmem
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=5:00:00
    #SBATCH --mem=250GB
    
    # Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    For i in *fastq;
    Do
	    dedupe2.sh in=$i \
	    out=(data output directory)/${i%.fa*}_deduped.fastq \
	    ac=f;
	    Done 

This resulted in all remaining reads being located in the dedupe output directory. It was then possible to quantify the number of reads that were lost during the deduplication process, allowing observation of the proportion of duplicated reads initially in the dataset.

#### Program Version used:


## 2. Low Complexity Removal
This step was completed using a program known as Komplexity which is able to be downloaded via miniconda using the following command: 

This step involves the removal of any samples that have low complexity (as determined as below the threshold of (()(): this was the default selected). The removal of low complexity sequences will result in more concise matching to the annotated sequences further down the processing pipeline. 

* I would recommend before running the following commands, copying the output of the dedupe into a seperate and safe file as komplexity does not make a new output directory, rather replaces the input through filtering
* komplexity also does not work on any zipped files. To unzip any files use the following command:

        gunzip *fastq.gz


#### Komplexity was installed via:

    conda install -eclarke -n Komplex komplexity

#### To run the reads the following commands were entered:

    source activate Komplex
    sbatch (jobscript name)
    
#### The script to be entered is:

    #!/bin/bash
    #SBATCH -p highmem
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=3:00:00
    #SBATCH --mem=200GB
    
    # Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    For fq in (input directory)/*.fastq;
    do
    kz --filter < $fq > ${fq/.fastq/-kz.fastq};
    done
    
This resulted in a only a return of high complexity files. It was then possible to quantify the number of reads lost during the removal of low complexity reads phase to determine the proportion of the dataset that was low proportion.

#### Program version used:

## 3. Human DNA removal 

### This step used 'kneaddata' which was downloaded through the following scripts:

This step is used to remove and human DNA from the sequences. this will ensure that the sequences ultimately used contain only viral, bacterial and archaic sequences, so as to ensure that there is lowered/no contamination present in the final dataset

#### kneaddata was installed via:

    conda create -n knead kneaddata p=3
    
#### To  run the reads the following commands were entered:

    source activate knead
    module load parallel
    sbatch (Jobscript name)
    
#### The script to be entered is:

    #!/bin/bash
    #SBATCH -p sbatch
    #SBATCH -N 1
    #SBATCH -n 16
    #SBATCH --time=10:00:00
    #SBATCH --mem=64GB
    
    #Notification configuration
    #SBATCH --mail-type=END
    #SBATCH --mail-type=FAIL
    #SBATCH --mail-user=(email address)
    
    parallel -j  1  'kneaddata --link -i {1(links to input for read 1)} -1 {2(links to input for read 2)} \
    -o /(output directory)/ \
    -db /(database entry for homo sapiens)/ \
    --remove-intermediate-output --bypass-trim -p 10 -t 32'\
    ::: /(input for read 1) \
    ::: /(input for read 2) \
    
#### Program version used:

## 4. Quality Assessment
This step was compleed using a FastQ and MultiQC. These were able to be loaded to miniconda using the following commands:

This step was used to assess the quality of the sequences so as to ensure that the sequences are appropriate to continue with. This step allows for the identification of quality based on the following elements: 

### This step was completed using the following scripts:

#### Fastq was already installed as part of the ())( package: To confirm that it is already present run the following command:

    module spider FastQC

#### To run the reads the following commands were entered:

    module load FastQC

#### The script to be entered is:

    fastqc -t -o (input)/ (output)/*
    
#### Program version used:


